# =============================================================================
# Sample Miner API v3.0 - Environment Configuration
# =============================================================================
# Copy this file to .env and update with your actual values
# File: .env.example

# =============================================================================
# API Server Configuration
# =============================================================================
# REQUIRED: API authentication key (required for all endpoints except /health)
# Generate a secure key with: python -c "import secrets; print(secrets.token_urlsafe(32))"
# NEVER use the default in production!
API_KEY=your-secure-random-api-key-here

# Server port
PORT=8001

# Server host (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
HOST=0.0.0.0

# Environment mode (production, development, staging)
# Use 'production' for deployed instances
ENVIRONMENT=production

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Options: "openai" or "vllm"
# - openai: Use OpenAI's API (requires API key, RECOMMENDED for most users)
# - vllm: Use self-hosted vLLM server (requires GPU hardware)
LLM_PROVIDER=openai

# =============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# =============================================================================
# REQUIRED: Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-actual-openai-api-key-here

# Model to use (recommended: gpt-4o-mini for cost efficiency)
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
OPENAI_MODEL=gpt-4o-mini

# Optional: Custom OpenAI base URL (for Azure OpenAI, proxies, etc.)
# OPENAI_BASE_URL=https://api.openai.com/v1

# =============================================================================
# vLLM Configuration (when LLM_PROVIDER=vllm)
# =============================================================================
# URL of your vLLM server
VLLM_BASE_URL=http://localhost:8000/v1

# Model deployed in vLLM
# Examples:
# - hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4 (4-5GB VRAM, quantized)
# - meta-llama/Llama-3.2-3B-Instruct (6-8GB VRAM)
# - meta-llama/Llama-3.1-8B-Instruct (16GB VRAM)
VLLM_MODEL=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4

# vLLM API key (set if your vLLM server requires authentication, otherwise leave empty)
VLLM_API_KEY=

# =============================================================================
# Conversation History Settings
# =============================================================================
# Maximum messages to store per conversation
MAX_CONVERSATION_MESSAGES=10

# Days before old messages are automatically deleted
CONVERSATION_CLEANUP_DAYS=7

# Number of recent messages to send to LLM (for token efficiency)
# Stores 10, but only sends 5 most recent to save tokens
SMART_HISTORY_COUNT=5

# =============================================================================
# Performance & Optimization Settings
# =============================================================================
# HTTP connection pool settings (for better performance)
CONNECTION_POOL_KEEPALIVE=20
CONNECTION_POOL_MAX=100
CONNECTION_POOL_KEEPALIVE_EXPIRY=30

# Request timeout in seconds
REQUEST_TIMEOUT=60

# =============================================================================
# Rate Limiting (optional, defaults shown)
# =============================================================================
# These are set in code but can be overridden:
# - Complete/Refine/Feedback: 20 requests/minute
# - Human Feedback: 30 requests/minute
# - Summary/Aggregate: 15 requests/minute
# - Internet Search: 10 requests/minute

# =============================================================================
# Input Validation Limits (optional, defaults shown)
# =============================================================================
# These are set in code but documented here:
# - user_query: max 10,000 characters
# - notebook: max 50,000 characters
# - input items: max 50 per request
# - previous_outputs: max 20 per request

# =============================================================================
# Logging Configuration
# =============================================================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable debug mode (more verbose logging)
DEBUG=false

# =============================================================================
# Advanced Model Configuration (optional)
# =============================================================================
# Maximum tokens in completion
# MAX_TOKENS=4000

# Sampling temperature (0.0 - 2.0, higher = more creative)
# TEMPERATURE=0.7

# Top-p sampling (0.0 - 1.0)
# TOP_P=0.9

# Frequency penalty (-2.0 - 2.0)
# FREQUENCY_PENALTY=0.0

# Presence penalty (-2.0 - 2.0)
# PRESENCE_PENALTY=0.0

# =============================================================================
# Database Configuration (PostgreSQL)
# =============================================================================
# PostgreSQL connection string
# Format: postgresql://username:password@host:port/database
# Database Configuration (SQLite)
# SQLite database file location (relative or absolute path)
DATABASE_URL=sqlite:///./data/miner_api.db
# For PostgreSQL (if you prefer): postgresql://postgres:postgres@localhost:5432/miner_api

# Connection pool settings
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20
DATABASE_POOL_RECYCLE=3600

# =============================================================================
# Gradio Test UI Configuration (optional)
# =============================================================================
# Used by gradio_test_ui_v3.py
API_BASE_URL=http://localhost:8001

# =============================================================================
# Installation Notes
# =============================================================================
# 
# MINIMAL INSTALL (OpenAI only - RECOMMENDED):
#   pip install -r requirements-minimal.txt
#   Set LLM_PROVIDER=openai
#   Set OPENAI_API_KEY=sk-your-key
#
# FULL INSTALL (with vLLM support):
#   pip install -r requirements.txt
#   Requires: CUDA 11.8+, GPU with 4GB+ VRAM
#   Set LLM_PROVIDER=vllm
#   Set VLLM_BASE_URL and VLLM_MODEL
#
# Documentation:
# - README.md - Full API documentation
# - INSTALL.md - Quick installation guide
# - DEPENDENCY_GUIDE.md - Dependency details
# - GRADIO_UI.md - Test UI documentation
#
# =============================================================================
